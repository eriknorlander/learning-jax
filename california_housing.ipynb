{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9q9JZjt16AMf"
   },
   "source": [
    "# Classic Supervised Examples in Flax\n",
    "\n",
    "This notebook is based on Flax's official [Annoted MNIST](https://flax.readthedocs.io/en/latest/notebooks/annotated_mnist.html)\n",
    "and [MNIST Example](https://github.com/google/flax/tree/main/examples/mnist).\n",
    "\n",
    "But has been written to be a slightly more general Supervised Classification example. \n",
    "Primarily for educational purposes and reusability in other examples.\n",
    "\n",
    "A really nice thing about this example is the use of the TrainState-object. We are able to bundle many necessary variables and functions with this object and can simply pass that around to our different functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSaA8Mif6srP"
   },
   "source": [
    "## Imports\n",
    "\n",
    "Import JAX, [JAX NumPy](https://jax.readthedocs.io/en/latest/jax.numpy.html),\n",
    "Flax, ordinary NumPy, and TensorFlow Datasets (TFDS). Flax can use any\n",
    "data-loading pipeline and this example demonstrates how to utilize TFDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Z7MuGFB16E8m"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp                # JAX NumPy\n",
    "\n",
    "from flax import linen as nn           # The Linen API\n",
    "from flax.training import train_state  # Useful dataclass to keep train state\n",
    "\n",
    "import numpy as np                     # Ordinary NumPy\n",
    "import optax                           # Optimizers\n",
    "import tensorflow_datasets as tfds     # TFDS for MNIST\n",
    "from ipywidgets import IntProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AuaRYtsKmHlm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<jaxlib.xla_extension.Device at 0x7f7f80561cb0>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset - California Housing Regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "IOeWiS_b-p8O"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from jax import numpy as jnp\n",
    "\n",
    "def to_jax(data):\n",
    "    return jnp.array(data, dtype=jnp.float32)\n",
    "\n",
    "def get_datasets():\n",
    "    \"\"\"Load breast cancer train and test datasets into memory.\"\"\"\n",
    "    X, y = datasets.fetch_california_housing(return_X_y=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)\n",
    "    \n",
    "    # Normalize\n",
    "    mean = X_train.mean(axis=0)\n",
    "    std = X_train.std(axis=0)\n",
    "    X_train = (X_train - mean) / std\n",
    "    X_test = (X_test - mean) / std\n",
    "    \n",
    "    train_ds = {'X' : to_jax(X_train), 'y' : to_jax(y_train)}\n",
    "    test_ds = {'X' : to_jax(X_test), 'y' : to_jax(y_test)}\n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = get_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4128, 8)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds['X'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0FW1DHa6cfH"
   },
   "source": [
    "\n",
    "## 2. Define network\n",
    "\n",
    "Create a simple MLP neural network with the Linen API by subclassing\n",
    "[`Module`](https://flax.readthedocs.io/en/latest/flax.linen.html#core-module-abstraction).\n",
    "\n",
    "Here, two implementaions are provided: \n",
    "1. Using the [`@compact`](https://flax.readthedocs.io/en/latest/flax.linen.html#compact-methods) decorator. This is a very neat and \"Keras\"-like way of writing it.\n",
    "2. Without `@compact`. Now you have to implement the `setup`-method, but it gives you more flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "class MLPRegressor(nn.Module):\n",
    "    \"\"\"A simple MLP model\"\"\"\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=16)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=8)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=4)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=1)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPRegressor(nn.Module):\n",
    "    dimensions: Sequence[int]\n",
    "\n",
    "    def setup(self):\n",
    "        self.layers = [nn.Dense(d) for d in self.dimensions]\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for i, l in enumerate(self.layers):\n",
    "            x = l(x)\n",
    "            if i != len(self.layers) - 1:\n",
    "                x = nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating training state\n",
    "This makes it much easier in traing as you only need to pass around the TrainState object in training and inference.\n",
    "\n",
    "Because this is such a common pattern, Flax provides the class\n",
    "[flax.training.train_state.TrainState](https://flax.readthedocs.io/en/latest/flax.training.html#train-state)\n",
    "that serves most basic usecases. Usually one would subclass it to add more data\n",
    "to be tracked, but in this example we can use it without any modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "2KOCnNJtdfTK"
   },
   "outputs": [],
   "source": [
    "def create_train_state(model, rng, learning_rate, momentum, sample):\n",
    "    params = model.init(rng, jnp.ones(np.shape([sample])))['params']\n",
    "    tx = optax.sgd(learning_rate, momentum)\n",
    "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple help-function for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_layers(params):\n",
    "    for layer_params in params.items():\n",
    "        print(\"Layer Name : {}\".format(layer_params[0]))\n",
    "        weights, biases = layer_params[1][\"kernel\"], layer_params[1][\"bias\"]\n",
    "        print(\"\\tLayer Weights : {}, Biases : {}\".format(weights.shape, biases.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define a predict method\n",
    "Since the output of the model is a sigmoid, it is already a probability. No need for softmax layers like we had with MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def predict(state, X):\n",
    "    y_pred = state.apply_fn({'params' : state.params}, X)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define a loss function and evaluation function\n",
    "We use classic negative log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_loss(preds, labels):\n",
    "    preds = preds.squeeze()\n",
    "    return (-labels * jnp.log(preds) - (1 - labels) * jnp.log(1 - preds)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(preds, labels):\n",
    "    return jnp.power(labels - preds.squeeze(), 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried with  \n",
    "\n",
    "```[0 if pred < 0.5 else 1 for pred in preds] ``` \n",
    "\n",
    "first but it turns out JAX needs to know the sizes of all arrays at compilation time. Therefore,  ```jnp.where() ``` was used.\n",
    "\n",
    "https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "KvuEA8Tw-MYa"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(*, preds, labels):\n",
    "    loss = mse_loss(preds=preds, labels=labels)\n",
    "    accuracy = jnp.mean(jnp.where(preds < .5, 0, 1).squeeze() == labels)\n",
    "    metrics = {\n",
    "        'loss': loss\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "w1J9i6alBv_u"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def eval_step(state, X, y):\n",
    "    preds = predict(state, X)\n",
    "    return compute_metrics(preds=preds, labels=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe include loss function in the training state? Would make the compute_metrics-function more general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7l-75YE-sr-"
   },
   "source": [
    "## 6. Training step\n",
    "\n",
    "A function that:\n",
    "\n",
    "- Evaluates the neural network given the parameters and a batch of input images\n",
    "  with the\n",
    "  [`Module.apply`](https://flax.readthedocs.io/en/latest/flax.linen.html#flax.linen.Module.apply)\n",
    "  method.\n",
    "- Computes the `neg_log_loss` loss function.\n",
    "- Evaluates the loss function and its gradient using\n",
    "  [`jax.value_and_grad`](https://jax.readthedocs.io/en/latest/jax.html#jax.value_and_grad).\n",
    "- Applies a\n",
    "  [pytree](https://jax.readthedocs.io/en/latest/pytrees.html#pytrees-and-jax-functions)\n",
    "  of gradients to the optimizer to update the model's parameters.\n",
    "- Computes the metrics using `compute_metrics` (defined earlier).\n",
    "\n",
    "Use JAX's [`@jit`](https://jax.readthedocs.io/en/latest/jax.html#jax.jit)\n",
    "decorator to trace the entire `train_step` function and just-in-time compile\n",
    "it with [XLA](https://www.tensorflow.org/xla) into fused device operations\n",
    "that run faster and more efficiently on hardware accelerators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "Ng11cdMf-z0x"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, X, y):\n",
    "    \"\"\"Train for a single step.\"\"\"\n",
    "    def loss_fn(params):\n",
    "        preds = state.apply_fn({'params' : params}, X)\n",
    "        loss = mse_loss(preds=preds, labels=y)\n",
    "        return loss, preds\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (_, preds), grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics = compute_metrics(preds=preds, labels=y)\n",
    "    return state, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBTLQPC4BxgH"
   },
   "source": [
    "## 7. Train function\n",
    "\n",
    "Define a training function that:\n",
    "\n",
    "- Shuffles the training data before each epoch using\n",
    "  [`jax.random.permutation`](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.permutation.html)\n",
    "  that takes a PRNGKey as a parameter (check the\n",
    "  [JAX - the sharp bits](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#JAX-PRNG)).\n",
    "- Runs an optimization step for each batch.\n",
    "- Retrieves the training metrics from the device with `jax.device_get` and\n",
    "  computes their mean across each batch in an epoch.\n",
    "- Returns the optimizer with updated parameters and the training loss and\n",
    "  accuracy metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "7ipyJ-JGCNqP"
   },
   "outputs": [],
   "source": [
    "def train_epoch(state, train_ds, batch_size, epoch, rng):\n",
    "    \"\"\"Train for a single epoch.\"\"\"\n",
    "    train_ds_size = len(train_ds['X'])\n",
    "    steps_per_epoch = train_ds_size // batch_size\n",
    "    \n",
    "    perms = jax.random.permutation(rng, train_ds_size)\n",
    "    perms = perms[:steps_per_epoch * batch_size]  # skip incomplete batch\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size)) # perms are indices \n",
    "    batch_metrics = []\n",
    "    for perm in perms:\n",
    "        batch = {k: v[perm] for k, v in train_ds.items()} # v[perm, ...] ? \n",
    "        \n",
    "        # This updates the state!\n",
    "        state, metrics = train_step(state, X=batch['X'], y=batch['y'])\n",
    "        batch_metrics.append(metrics)\n",
    "\n",
    "    # compute mean of metrics across each batch in epoch.\n",
    "    batch_metrics_np = jax.device_get(batch_metrics)\n",
    "    epoch_metrics_np = {k: np.mean([metrics[k] \n",
    "                        for metrics in batch_metrics_np])\n",
    "                        for k in batch_metrics_np[0]}\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2cHbVUfCRMv"
   },
   "source": [
    "## 8. Eval function\n",
    "\n",
    "Create a model evaluation function that:\n",
    "\n",
    "- Retrieves the evaluation metrics from the device with `jax.device_get`.\n",
    "- Copies the metrics\n",
    "  [data stored](https://flax.readthedocs.io/en/latest/design_notes/linen_design_principles.html#how-are-parameters-represented-and-how-do-we-handle-general-differentiable-algorithms-that-update-stateful-variables)\n",
    "  in a JAX\n",
    "  [pytree](https://jax.readthedocs.io/en/latest/pytrees.html#pytrees-and-jax-functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "_dKahNmMCr5q"
   },
   "outputs": [],
   "source": [
    "def eval_model(state, X, y):\n",
    "    metrics = eval_step(state, X, y)\n",
    "    metrics = jax.device_get(metrics)\n",
    "    summary = jax.tree_map(lambda x: x.item(), metrics)\n",
    "    return summary['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name : layers_0\n",
      "\tLayer Weights : (8, 16), Biases : (16,)\n",
      "Layer Name : layers_1\n",
      "\tLayer Weights : (16, 8), Biases : (8,)\n",
      "Layer Name : layers_2\n",
      "\tLayer Weights : (8, 4), Biases : (4,)\n",
      "Layer Name : layers_3\n",
      "\tLayer Weights : (4, 1), Biases : (1,)\n"
     ]
    }
   ],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "model = MLPRegressor([16,8,4,1])\n",
    "sample = train_ds['X'][0]\n",
    "learning_rate = 0.0001\n",
    "momentum = 0.9\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "# Create Training State\n",
    "state = create_train_state(model, rng, learning_rate, momentum, sample)\n",
    "#del seed\n",
    "\n",
    "# Visualize layers\n",
    "visualize_layers(state.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_ds['X']\n",
    "y_train = train_ds['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = train_epoch(state, train_ds, batch_size, epoch=1, rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(state, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(4.0555077, dtype=float32)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_loss(y_pred, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqNrWu7kIC9S"
   },
   "source": [
    "## 10. Train and evaluate\n",
    "\n",
    "Once the training and testing is done after 10 epochs, the output should show that your model was able to achieve approximately 99% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "ugGlV3u6Iq1A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1   | loss: 2.82406 | val_loss: 2.74745 |\n",
      "epoch: 2   | loss: 2.12478 | val_loss: 2.06187 |\n",
      "epoch: 3   | loss: 1.73974 | val_loss: 1.68698 |\n",
      "epoch: 4   | loss: 1.53397 | val_loss: 1.48911 |\n",
      "epoch: 5   | loss: 1.42925 | val_loss: 1.38987 |\n",
      "epoch: 6   | loss: 1.37640 | val_loss: 1.34094 |\n",
      "epoch: 7   | loss: 1.34821 | val_loss: 1.31531 |\n",
      "epoch: 8   | loss: 1.32991 | val_loss: 1.29884 |\n",
      "epoch: 9   | loss: 1.31678 | val_loss: 1.28694 |\n",
      "epoch: 10  | loss: 1.30540 | val_loss: 1.27628 |\n",
      "epoch: 11  | loss: 1.29500 | val_loss: 1.26629 |\n",
      "epoch: 12  | loss: 1.28469 | val_loss: 1.25616 |\n",
      "epoch: 13  | loss: 1.27379 | val_loss: 1.24536 |\n",
      "epoch: 14  | loss: 1.26187 | val_loss: 1.23351 |\n",
      "epoch: 15  | loss: 1.24857 | val_loss: 1.22025 |\n",
      "epoch: 16  | loss: 1.23355 | val_loss: 1.20516 |\n",
      "epoch: 17  | loss: 1.21642 | val_loss: 1.18787 |\n",
      "epoch: 18  | loss: 1.19654 | val_loss: 1.16778 |\n",
      "epoch: 19  | loss: 1.17271 | val_loss: 1.14378 |\n",
      "epoch: 20  | loss: 1.14334 | val_loss: 1.11401 |\n",
      "epoch: 21  | loss: 1.10877 | val_loss: 1.07892 |\n",
      "epoch: 22  | loss: 1.06875 | val_loss: 1.03841 |\n",
      "epoch: 23  | loss: 1.02330 | val_loss: 0.99271 |\n",
      "epoch: 24  | loss: 0.97248 | val_loss: 0.94221 |\n",
      "epoch: 25  | loss: 0.91824 | val_loss: 0.88906 |\n",
      "epoch: 26  | loss: 0.86386 | val_loss: 0.83697 |\n",
      "epoch: 27  | loss: 0.81135 | val_loss: 0.78665 |\n",
      "epoch: 28  | loss: 0.76541 | val_loss: 0.74295 |\n",
      "epoch: 29  | loss: 0.72754 | val_loss: 0.70786 |\n",
      "epoch: 30  | loss: 0.69821 | val_loss: 0.68082 |\n",
      "epoch: 31  | loss: 0.67717 | val_loss: 0.66284 |\n",
      "epoch: 32  | loss: 0.66177 | val_loss: 0.64976 |\n",
      "epoch: 33  | loss: 0.65022 | val_loss: 0.64077 |\n",
      "epoch: 34  | loss: 0.64129 | val_loss: 0.63383 |\n",
      "epoch: 35  | loss: 0.63405 | val_loss: 0.62817 |\n",
      "epoch: 36  | loss: 0.62782 | val_loss: 0.62341 |\n",
      "epoch: 37  | loss: 0.62233 | val_loss: 0.61900 |\n",
      "epoch: 38  | loss: 0.61721 | val_loss: 0.61499 |\n",
      "epoch: 39  | loss: 0.61265 | val_loss: 0.61132 |\n",
      "epoch: 40  | loss: 0.60829 | val_loss: 0.60771 |\n",
      "epoch: 41  | loss: 0.60413 | val_loss: 0.60418 |\n",
      "epoch: 42  | loss: 0.60012 | val_loss: 0.60078 |\n",
      "epoch: 43  | loss: 0.59637 | val_loss: 0.59745 |\n",
      "epoch: 44  | loss: 0.59268 | val_loss: 0.59426 |\n",
      "epoch: 45  | loss: 0.58920 | val_loss: 0.59112 |\n",
      "epoch: 46  | loss: 0.58578 | val_loss: 0.58810 |\n",
      "epoch: 47  | loss: 0.58254 | val_loss: 0.58512 |\n",
      "epoch: 48  | loss: 0.57938 | val_loss: 0.58221 |\n",
      "epoch: 49  | loss: 0.57633 | val_loss: 0.57944 |\n",
      "epoch: 50  | loss: 0.57335 | val_loss: 0.57675 |\n",
      "epoch: 51  | loss: 0.57040 | val_loss: 0.57395 |\n",
      "epoch: 52  | loss: 0.56753 | val_loss: 0.57136 |\n",
      "epoch: 53  | loss: 0.56467 | val_loss: 0.56877 |\n",
      "epoch: 54  | loss: 0.56188 | val_loss: 0.56612 |\n",
      "epoch: 55  | loss: 0.55904 | val_loss: 0.56356 |\n",
      "epoch: 56  | loss: 0.55628 | val_loss: 0.56104 |\n",
      "epoch: 57  | loss: 0.55352 | val_loss: 0.55840 |\n",
      "epoch: 58  | loss: 0.55072 | val_loss: 0.55578 |\n",
      "epoch: 59  | loss: 0.54798 | val_loss: 0.55330 |\n",
      "epoch: 60  | loss: 0.54525 | val_loss: 0.55080 |\n",
      "epoch: 61  | loss: 0.54254 | val_loss: 0.54835 |\n",
      "epoch: 62  | loss: 0.53981 | val_loss: 0.54584 |\n",
      "epoch: 63  | loss: 0.53705 | val_loss: 0.54346 |\n",
      "epoch: 64  | loss: 0.53440 | val_loss: 0.54109 |\n",
      "epoch: 65  | loss: 0.53167 | val_loss: 0.53873 |\n",
      "epoch: 66  | loss: 0.52891 | val_loss: 0.53631 |\n",
      "epoch: 67  | loss: 0.52626 | val_loss: 0.53392 |\n",
      "epoch: 68  | loss: 0.52374 | val_loss: 0.53158 |\n",
      "epoch: 69  | loss: 0.52119 | val_loss: 0.52934 |\n",
      "epoch: 70  | loss: 0.51848 | val_loss: 0.52719 |\n",
      "epoch: 71  | loss: 0.51570 | val_loss: 0.52501 |\n",
      "epoch: 72  | loss: 0.51317 | val_loss: 0.52297 |\n",
      "epoch: 73  | loss: 0.51072 | val_loss: 0.52101 |\n",
      "epoch: 74  | loss: 0.50828 | val_loss: 0.51916 |\n",
      "epoch: 75  | loss: 0.50589 | val_loss: 0.51722 |\n",
      "epoch: 76  | loss: 0.50363 | val_loss: 0.51544 |\n",
      "epoch: 77  | loss: 0.50149 | val_loss: 0.51385 |\n",
      "epoch: 78  | loss: 0.49941 | val_loss: 0.51210 |\n",
      "epoch: 79  | loss: 0.49740 | val_loss: 0.51038 |\n",
      "epoch: 80  | loss: 0.49541 | val_loss: 0.50879 |\n",
      "epoch: 81  | loss: 0.49342 | val_loss: 0.50723 |\n",
      "epoch: 82  | loss: 0.49154 | val_loss: 0.50565 |\n",
      "epoch: 83  | loss: 0.48974 | val_loss: 0.50415 |\n",
      "epoch: 84  | loss: 0.48799 | val_loss: 0.50271 |\n",
      "epoch: 85  | loss: 0.48639 | val_loss: 0.50116 |\n",
      "epoch: 86  | loss: 0.48457 | val_loss: 0.49980 |\n",
      "epoch: 87  | loss: 0.48282 | val_loss: 0.49813 |\n",
      "epoch: 88  | loss: 0.48122 | val_loss: 0.49680 |\n",
      "epoch: 89  | loss: 0.47961 | val_loss: 0.49535 |\n",
      "epoch: 90  | loss: 0.47806 | val_loss: 0.49389 |\n",
      "epoch: 91  | loss: 0.47654 | val_loss: 0.49254 |\n",
      "epoch: 92  | loss: 0.47502 | val_loss: 0.49107 |\n",
      "epoch: 93  | loss: 0.47358 | val_loss: 0.48975 |\n",
      "epoch: 94  | loss: 0.47213 | val_loss: 0.48847 |\n",
      "epoch: 95  | loss: 0.47073 | val_loss: 0.48723 |\n",
      "epoch: 96  | loss: 0.46944 | val_loss: 0.48594 |\n",
      "epoch: 97  | loss: 0.46807 | val_loss: 0.48475 |\n",
      "epoch: 98  | loss: 0.46680 | val_loss: 0.48345 |\n",
      "epoch: 99  | loss: 0.46559 | val_loss: 0.48219 |\n",
      "epoch: 100 | loss: 0.46443 | val_loss: 0.48110 |\n"
     ]
    }
   ],
   "source": [
    "history = {'loss' : [], 'val_loss' : [], 'epochs' : []}\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Use a separate PRNG key to permute image data during shuffling\n",
    "    rng, input_rng = jax.random.split(rng)\n",
    "    # Run an optimization step over a training batch\n",
    "    state = train_epoch(state, train_ds, batch_size, epoch, input_rng)\n",
    "    # Evaluate on the test set after each training epoch \n",
    "    loss = eval_model(state, X=train_ds['X'], y=train_ds['y'])\n",
    "    val_loss = eval_model(state, X=test_ds['X'], y=test_ds['y'])\n",
    "    history['loss'].append(loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['epochs'].append(epoch)\n",
    "    if epoch < 10:\n",
    "        print('epoch: {}   | loss: {:.5f} | val_loss: {:.5f} |'.format(epoch, loss, val_loss))\n",
    "    elif epoch < 100:\n",
    "        print('epoch: {}  | loss: {:.5f} | val_loss: {:.5f} |'.format(epoch, loss, val_loss))\n",
    "    else:\n",
    "        print('epoch: {} | loss: {:.5f} | val_loss: {:.5f} |'.format(epoch, loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.apply of MLPRegressor(\n",
       "    # attributes\n",
       "    dimensions = [16, 8, 4, 1]\n",
       ")>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.apply_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['step', 'apply_fn', 'params', 'tx', 'opt_state'])\n"
     ]
    }
   ],
   "source": [
    "# apply_fn was defined above to be CNN().apply()\n",
    "print(state.__dict__.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FNTxLXI8kDeK",
    "outputId": "4fd45fad-3454-4ab8-f766-55d5f56b41ec"
   },
   "outputs": [],
   "source": [
    "y_test = test_ds['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "giIkry0Ck-zY",
    "outputId": "c987f6e8-af75-47dc-cf00-33b3d01d56c2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history['epochs'], history['loss'], label='Loss')\n",
    "plt.plot(history['epochs'], history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Official MNIST FLAX",
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,md:myst",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
